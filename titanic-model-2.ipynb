{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-11T19:01:15.133020Z","iopub.execute_input":"2022-07-11T19:01:15.134649Z","iopub.status.idle":"2022-07-11T19:01:15.179602Z","shell.execute_reply.started":"2022-07-11T19:01:15.134453Z","shell.execute_reply":"2022-07-11T19:01:15.178419Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"titanic_file_path = '/kaggle/input/titanic/train.csv'\n\ntitanic_data = pd.read_csv(titanic_file_path)\n\ntitanic_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.181775Z","iopub.execute_input":"2022-07-11T19:01:15.182879Z","iopub.status.idle":"2022-07-11T19:01:15.230452Z","shell.execute_reply.started":"2022-07-11T19:01:15.182815Z","shell.execute_reply":"2022-07-11T19:01:15.229102Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"First, I will make a list of all the columns so I can take a look at the correlation matrix for each of these elements.","metadata":{}},{"cell_type":"code","source":"columns = list(titanic_data.columns.values)\nprint(columns)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.232333Z","iopub.execute_input":"2022-07-11T19:01:15.233385Z","iopub.status.idle":"2022-07-11T19:01:15.240352Z","shell.execute_reply.started":"2022-07-11T19:01:15.233326Z","shell.execute_reply":"2022-07-11T19:01:15.238520Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"With the elements listed, I now need to see which are numeric and which are categorical, which I can do with describe().","metadata":{}},{"cell_type":"code","source":"print(titanic_data.describe())","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.243470Z","iopub.execute_input":"2022-07-11T19:01:15.244157Z","iopub.status.idle":"2022-07-11T19:01:15.295681Z","shell.execute_reply.started":"2022-07-11T19:01:15.244118Z","shell.execute_reply":"2022-07-11T19:01:15.294734Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Now I will need to review how each of the remaining elements are categorized to see if they, perhaps, can logically be split into larger groups for comparison.","metadata":{}},{"cell_type":"code","source":"categorical_subset = titanic_data[['Sex', 'Embarked', 'Ticket', 'Cabin']]\n\nprint(titanic_data.Sex.head(15))\nprint(titanic_data.Embarked.head(15))\nprint(titanic_data.Ticket.head(15))\nprint(titanic_data.Cabin.head(15))","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.297041Z","iopub.execute_input":"2022-07-11T19:01:15.297607Z","iopub.status.idle":"2022-07-11T19:01:15.309379Z","shell.execute_reply.started":"2022-07-11T19:01:15.297571Z","shell.execute_reply":"2022-07-11T19:01:15.308114Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n\nprint(titanic_data['Sex'].value_counts())\nprint(titanic_data['Embarked'].value_counts())\nprint(titanic_data['Ticket'].value_counts())\nprint(titanic_data['Cabin'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.311119Z","iopub.execute_input":"2022-07-11T19:01:15.311993Z","iopub.status.idle":"2022-07-11T19:01:15.333160Z","shell.execute_reply.started":"2022-07-11T19:01:15.311939Z","shell.execute_reply":"2022-07-11T19:01:15.331430Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(titanic_data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.334992Z","iopub.execute_input":"2022-07-11T19:01:15.336225Z","iopub.status.idle":"2022-07-11T19:01:15.351851Z","shell.execute_reply.started":"2022-07-11T19:01:15.336177Z","shell.execute_reply":"2022-07-11T19:01:15.349983Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Clearly, Sex is an easily categorized variable, as is Embarked, since those contain a minimum number of possible responses. Those can be assigned dummy values as they are. Ticket does not have any immediately noticeable patterns, though there are occasional repeating letter patterns. Additional exploration would be necessary to use that variable.\n\nCabin provides an interesting possibility for consideration. The naming convension of the cabins indicates that they are separated into sections by letter. It is possible that certain sections were more deadly than others, regardless of Sex of Pclass. It is also notable that Cabin has the most null values in the data set by a large margin. Both of these facts deserve exploration.","metadata":{}},{"cell_type":"code","source":"import re\n\ntitanic_data['cabin_section'] = titanic_data['Cabin'].str.extract('([A-Z])', expand=True)\ntitanic_data['cabin_section'] = titanic_data[['cabin_section']].fillna(value = 'Unknown')\n\n# pd.set_option(\"max_rows\", None)\n\ntitanic_data['cabin_section'].head(20)\n# titanic_data.Cabin.head","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.353947Z","iopub.execute_input":"2022-07-11T19:01:15.354516Z","iopub.status.idle":"2022-07-11T19:01:15.375579Z","shell.execute_reply.started":"2022-07-11T19:01:15.354462Z","shell.execute_reply":"2022-07-11T19:01:15.373575Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(titanic_data['cabin_section'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.379837Z","iopub.execute_input":"2022-07-11T19:01:15.380227Z","iopub.status.idle":"2022-07-11T19:01:15.388728Z","shell.execute_reply.started":"2022-07-11T19:01:15.380195Z","shell.execute_reply":"2022-07-11T19:01:15.386710Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"I have altered the Cabin column to reflect the section of the ship where the cabin is located and turned this into a new variable cabin_section. I have also put NAN values into their own variable so they can be considered in the model. It is possible that this lack of information could constitute factors that might lead to surviving the disaster or not.","metadata":{}},{"cell_type":"code","source":"numerical_subset = titanic_data[['Pclass', 'Parch', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']]\n\ncategorical_subset = titanic_data[['Sex', 'Embarked', 'cabin_section']]\ncategorical_subset = pd.get_dummies(categorical_subset)\n\nfeatures = pd.concat([numerical_subset, categorical_subset], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.393741Z","iopub.execute_input":"2022-07-11T19:01:15.395852Z","iopub.status.idle":"2022-07-11T19:01:15.413927Z","shell.execute_reply.started":"2022-07-11T19:01:15.395778Z","shell.execute_reply":"2022-07-11T19:01:15.412643Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Find all correlations and sort \ncorrelations_data = features.corr()['Survived'].sort_values()\n\n# Print the most negative correlations\nprint(correlations_data.head(15), '\\n')\n\n# Print the most positive correlations\nprint(correlations_data.tail(15))","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.416468Z","iopub.execute_input":"2022-07-11T19:01:15.417503Z","iopub.status.idle":"2022-07-11T19:01:15.442022Z","shell.execute_reply.started":"2022-07-11T19:01:15.417431Z","shell.execute_reply":"2022-07-11T19:01:15.440792Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"From the correlation matrix, we can see that Sex has the largest correlation with survivorship. Class appears to be negatively associated with survivorship, but this is due to the computer assigned class values a true numerical value, rather than its symbolic one it has in the real world. cabin_section_Unknown is also negatively associated with survivorship, suggesting that we might be right about that variable.\n\nFare also seems to have played a role, as it is positively associated with survivorhood. This seems like an extension of Pclass, but if so, why is the correlation reduced by roughly 8%? Perhaps it is thrown off by the crew that did not pay for the voyage? Or perhaps some wealthy people were guests on the ship and did not pay. Both are possible, so additional exploration is required.\n\nWe will start by looking at a bar chart of survival rates by cabin section.","metadata":{}},{"cell_type":"code","source":"section_survival = titanic_data.groupby(['cabin_section'])['Survived'].value_counts().unstack().plot(figsize=(10, 8), kind='bar',stacked = False)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.443329Z","iopub.execute_input":"2022-07-11T19:01:15.443747Z","iopub.status.idle":"2022-07-11T19:01:15.801857Z","shell.execute_reply.started":"2022-07-11T19:01:15.443701Z","shell.execute_reply":"2022-07-11T19:01:15.800518Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"We can see here that survivor rates, with the exception of Sections A, G, and T, tend to be higher when the section is known. G and T seem to be fairly even, but not well represented.\n\nUpon further research (https://titanic.fandom.com/wiki/A_Deck), it appears that there are a lot of unknown occupants of Deck A, which might be contributing to its unusual composition. From the occupants that are known, it seems that many of these particular occupants were unwilling to leave, either due to disbelief, fear of leaving the ship, or guilt at its sinking, rather than being willfully excluded from lifeboats.\n\nDecks B and C https://titanic.fandom.com/wiki/B_Deck#:~:text=B%20Deck%2C%20also%20called%20the,featuring%20their%20own%20private%20promenades., https://titanic.fandom.com/wiki/C_Deck ), have a much higher proportion of known occupants than any other decks, so we can have a higher certainty that these proportions are fairly accurate.\n\nG and F decks were entirely third class passengers and both of these decks were completely flooded within minutes of hitting the iceberg. Since there were 1,100 third class passengers, it appears that both G and F were full of Unknown status passengers.\n\nInterestingly, E deck, despite being fairly low in the ship, had a comparatively high survival rate. Whether or not it played a part is uncertain, but we can note that a majority of the onboard crew were cabined on E deck (https://titanic.fandom.com/wiki/E_Deck ). It is possible that, despite a comparatively low status, their position of power on the ship afforded them a better chance of surviving.","metadata":{}},{"cell_type":"code","source":"sex_survival = titanic_data.groupby(['Sex'])['Survived'].value_counts().unstack().plot(figsize=(10, 8), kind='bar',stacked = False)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:15.803540Z","iopub.execute_input":"2022-07-11T19:01:15.803943Z","iopub.status.idle":"2022-07-11T19:01:16.020655Z","shell.execute_reply.started":"2022-07-11T19:01:15.803907Z","shell.execute_reply":"2022-07-11T19:01:16.019577Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"The difference between the survivorship of the sexes is highly significant, though unsurprising. The phrase \"women and children first\" seemed to have been a guiding cultural principle for those onboard, though it was clearly not absolute, as certain men were considered exceptions.","metadata":{}},{"cell_type":"code","source":"class_survival = titanic_data.groupby(['Pclass'])['Survived'].value_counts().unstack().plot(figsize=(10, 8), kind='bar',stacked = False)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:01:16.022397Z","iopub.execute_input":"2022-07-11T19:01:16.023829Z","iopub.status.idle":"2022-07-11T19:01:16.250719Z","shell.execute_reply.started":"2022-07-11T19:01:16.023786Z","shell.execute_reply":"2022-07-11T19:01:16.249671Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"The most significant piece of this graph is the likelihood of a third class passenger surviving, which was significantly lower than the other classes. As seen in the section examination above, part of this had to do with how quickly third class decks flooded. Many people on these decks simply did not have time to escape. The facts of the night make it clear, though, that this is not the only reason, as many third class passengers were denied access to lifeboats.\n\nWe may have a better approximation of the true significance by examining the difference between first and second class passengers, who were both on sections that flooded more slowly. Here, we can see that the difference is still stark, but not quite so extreme as with third class. Still, class exclusion becomes harsher as class level is lowered, so the answer is most likely somewhere in the middle.","metadata":{}},{"cell_type":"code","source":"#import seaborn as sns\n#sns.set(font_scale = 2)\n#import matplotlib.pyplot as plt\n#%matplotlib inline\n\n#cabin_encode = pd.get_dummies(titanic_data[['cabin_section']])\n#some_features = titanic_data[['Fare', 'Survived']]\n\n#features = pd.concat([some_features, cabin_encode], axis=1)\n\n# Use seaborn to plot a scatterplot of Score vs Log Source EUI\n#sns.lmplot('Fare', 'cabin_section', \n#          hue = 'Survived', data = features,\n#          scatter_kws = {'alpha': 0.8, 's': 60}, fit_reg = False,\n#          size = 12, aspect = 1.2);\n\n# Plot labeling\n#plt.xlabel(\"Fare\", size = 28)\n#plt.ylabel('Deck', size = 28)\n#plt.title('Class and Cabin Compared With Survival', size = 36);","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:39:38.749601Z","iopub.execute_input":"2022-07-11T19:39:38.750867Z","iopub.status.idle":"2022-07-11T19:39:38.755876Z","shell.execute_reply.started":"2022-07-11T19:39:38.750817Z","shell.execute_reply":"2022-07-11T19:39:38.754744Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"(This was an attempt to make use of a scatterplot, but this particular type of representation does not seem to serve our model, regardless of how it is changed. I may return to it later.)","metadata":{}},{"cell_type":"code","source":"numerical_subset = titanic_data[['Pclass', 'Fare']]\n\ncategorical_subset = titanic_data[['Sex', 'cabin_section', 'Embarked']]\ncategorical_subset = pd.get_dummies(categorical_subset)\n\nfeatures = pd.concat([numerical_subset, categorical_subset], axis=1)\n\nX = features\n\nprint(X.describe)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:47:05.204191Z","iopub.execute_input":"2022-07-11T19:47:05.204654Z","iopub.status.idle":"2022-07-11T19:47:05.228842Z","shell.execute_reply.started":"2022-07-11T19:47:05.204602Z","shell.execute_reply":"2022-07-11T19:47:05.227900Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"y = titanic_data.Survived","metadata":{"execution":{"iopub.status.busy":"2022-07-11T19:49:58.691532Z","iopub.execute_input":"2022-07-11T19:49:58.691937Z","iopub.status.idle":"2022-07-11T19:49:58.697219Z","shell.execute_reply.started":"2022-07-11T19:49:58.691906Z","shell.execute_reply":"2022-07-11T19:49:58.696154Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Data Prep\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Potential Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Testing\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# Final model decision\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-07-11T20:23:40.230789Z","iopub.execute_input":"2022-07-11T20:23:40.231190Z","iopub.status.idle":"2022-07-11T20:23:40.562551Z","shell.execute_reply.started":"2022-07-11T20:23:40.231155Z","shell.execute_reply":"2022-07-11T20:23:40.561157Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"After importing necessary libraries, we split the data into a training and test set.","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 13)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T20:24:56.527888Z","iopub.execute_input":"2022-07-11T20:24:56.528362Z","iopub.status.idle":"2022-07-11T20:24:56.538671Z","shell.execute_reply.started":"2022-07-11T20:24:56.528325Z","shell.execute_reply":"2022-07-11T20:24:56.537631Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Imputing the missing values of Age and Embark.","metadata":{}},{"cell_type":"code","source":"imputer = SimpleImputer(strategy='median')\n\nimputer.fit(X_train)\n\nX_train = imputer.transform(X_train)\nX_val = imputer.transform(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T20:36:51.066351Z","iopub.execute_input":"2022-07-11T20:36:51.067102Z","iopub.status.idle":"2022-07-11T20:36:51.079988Z","shell.execute_reply.started":"2022-07-11T20:36:51.067062Z","shell.execute_reply":"2022-07-11T20:36:51.078816Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print('Missing values in training features: ', np.sum(np.isnan(X_train)))\nprint('Missing values in testing features:  ', np.sum(np.isnan(X_val)))","metadata":{"execution":{"iopub.status.busy":"2022-07-11T20:37:41.031018Z","iopub.execute_input":"2022-07-11T20:37:41.031448Z","iopub.status.idle":"2022-07-11T20:37:41.038701Z","shell.execute_reply.started":"2022-07-11T20:37:41.031414Z","shell.execute_reply":"2022-07-11T20:37:41.037486Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Create the scaler object with a range of 0-1\nscaler = MinMaxScaler(feature_range=(0, 1))\n\n# Fit on the training data\nscaler.fit(X_train)\n\n# Transform both the training and testing data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T20:45:25.674708Z","iopub.execute_input":"2022-07-11T20:45:25.675861Z","iopub.status.idle":"2022-07-11T20:45:25.683279Z","shell.execute_reply.started":"2022-07-11T20:45:25.675814Z","shell.execute_reply":"2022-07-11T20:45:25.681919Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Convert y to one-dimensional array (vector)\ny_train = np.array(y_train).reshape((-1, ))\ny_val = np.array(y_val).reshape((-1, ))","metadata":{"execution":{"iopub.status.busy":"2022-07-11T20:46:28.101797Z","iopub.execute_input":"2022-07-11T20:46:28.102241Z","iopub.status.idle":"2022-07-11T20:46:28.110114Z","shell.execute_reply.started":"2022-07-11T20:46:28.102208Z","shell.execute_reply":"2022-07-11T20:46:28.108576Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Function to calculate mean absolute error\ndef mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))\n\n# Takes in a model, trains the model, and evaluates the model on the test set\ndef fit_and_evaluate(model):\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions and evalute\n    model_pred = model.predict(X_val)\n    model_mae = mae(y_val, model_pred)\n    \n    # Return the performance metric\n    return model_mae","metadata":{"execution":{"iopub.status.busy":"2022-07-11T21:16:24.207208Z","iopub.execute_input":"2022-07-11T21:16:24.207763Z","iopub.status.idle":"2022-07-11T21:16:24.215594Z","shell.execute_reply.started":"2022-07-11T21:16:24.207717Z","shell.execute_reply":"2022-07-11T21:16:24.214372Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"lr = LinearRegression()\nlr_mae = fit_and_evaluate(lr)\n\nprint('Linear Regression Performance on the test set: MAE = %0.4f' % lr_mae)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T21:16:26.421348Z","iopub.execute_input":"2022-07-11T21:16:26.421866Z","iopub.status.idle":"2022-07-11T21:16:26.452409Z","shell.execute_reply.started":"2022-07-11T21:16:26.421826Z","shell.execute_reply":"2022-07-11T21:16:26.451228Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"svm = SVR(C = 1000, gamma = 0.1)\nsvm_mae = fit_and_evaluate(svm)\n\nprint('Support Vector Machine Regression Performance on the test set: MAE = %0.4f' % svm_mae)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T21:16:41.821723Z","iopub.execute_input":"2022-07-11T21:16:41.822124Z","iopub.status.idle":"2022-07-11T21:16:42.958271Z","shell.execute_reply.started":"2022-07-11T21:16:41.822092Z","shell.execute_reply":"2022-07-11T21:16:42.956822Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"random_forest = RandomForestRegressor(random_state=60)\nrandom_forest_mae = fit_and_evaluate(random_forest)\n\nprint('Random Forest Regression Performance on the test set: MAE = %0.4f' % random_forest_mae)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T21:16:56.984338Z","iopub.execute_input":"2022-07-11T21:16:56.984858Z","iopub.status.idle":"2022-07-11T21:16:57.224833Z","shell.execute_reply.started":"2022-07-11T21:16:56.984816Z","shell.execute_reply":"2022-07-11T21:16:57.223399Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"gradient_boosted = GradientBoostingRegressor(random_state=60)\ngradient_boosted_mae = fit_and_evaluate(gradient_boosted)\n\nprint('Gradient Boosted Regression Performance on the test set: MAE = %0.4f' % gradient_boosted_mae)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T21:17:11.223423Z","iopub.execute_input":"2022-07-11T21:17:11.223921Z","iopub.status.idle":"2022-07-11T21:17:11.302109Z","shell.execute_reply.started":"2022-07-11T21:17:11.223881Z","shell.execute_reply":"2022-07-11T21:17:11.300969Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsRegressor(n_neighbors=10)\nknn_mae = fit_and_evaluate(knn)\n\nprint('K-Nearest Neighbors Regression Performance on the test set: MAE = %0.4f' % knn_mae)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T21:17:28.818442Z","iopub.execute_input":"2022-07-11T21:17:28.819330Z","iopub.status.idle":"2022-07-11T21:17:28.847980Z","shell.execute_reply.started":"2022-07-11T21:17:28.819282Z","shell.execute_reply":"2022-07-11T21:17:28.846567Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(20,10)) \n\n\n# Dataframe to hold the results\nmodel_comparison = pd.DataFrame({'model': ['Linear Regression', 'Support Vector Machine',\n                                           'Random Forest', 'Gradient Boosted',\n                                            'K-Nearest Neighbors'],\n                                 'mae': [lr_mae, svm_mae, random_forest_mae, \n                                         gradient_boosted_mae, knn_mae]})\n\n# Horizontal bar chart of test mae\nmodel_comparison.sort_values('mae', ascending = False).plot(x = 'model', y = 'mae', kind = 'barh',\n                                                           color = 'red', edgecolor = 'black')\n\n# Plot formatting\nplt.ylabel(''); plt.yticks(size = 14); plt.xlabel('Mean Absolute Error'); plt.xticks(size = 14)\nplt.title('Model Comparison on Test MAE', size = 20);","metadata":{"execution":{"iopub.status.busy":"2022-07-11T21:24:17.920860Z","iopub.execute_input":"2022-07-11T21:24:17.921280Z","iopub.status.idle":"2022-07-11T21:24:18.141675Z","shell.execute_reply.started":"2022-07-11T21:24:17.921247Z","shell.execute_reply":"2022-07-11T21:24:18.140278Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# With the model design complete, use all the data for the final model.\nrandom_forest.fit(X, y)\n\n# Prepare Test Data\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ntest_data['cabin_section'] = test_data['Cabin'].str.extract('([A-Z])', expand=True)\ntest_data['cabin_section'] = test_data[['cabin_section']].fillna(value = 'Unknown')\n\nnumerical_subset = test_data[['Pclass', 'Fare']]\n\ncategorical_subset = test_data[['Sex', 'cabin_section', 'Embarked']]\ncategorical_subset = pd.get_dummies(categorical_subset)\n\ntest_features = pd.concat([numerical_subset, categorical_subset], axis=1)\n\nX_test = test_features\n\ntrain_predictions = random_forest.predict(X_test).astype(int)\n\noutput = pd.DataFrame({'PassengerId' : test_data.PassengerId,\n                   'Survived' : test_predictions})\n\noutput.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-11T21:36:45.410979Z","iopub.execute_input":"2022-07-11T21:36:45.411483Z","iopub.status.idle":"2022-07-11T21:36:45.451969Z","shell.execute_reply.started":"2022-07-11T21:36:45.411444Z","shell.execute_reply":"2022-07-11T21:36:45.449567Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}